{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7b24f-d94c-4f43-a575-c78f95e7caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ucimlrepo\n",
    "#pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce3054-c43e-4cb1-8567-445962121dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from openai import OpenAI\n",
    "import os\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.explainers import MetricTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24829e81-44cb-4e9d-9c89-98aabfff5da7",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fbec74-acc4-4561-b7bc-f461df8ce03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 144, 'name': 'Statlog (German Credit Data)', 'repository_url': 'https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data', 'data_url': 'https://archive.ics.uci.edu/static/public/144/data.csv', 'abstract': 'This dataset classifies people described by a set of attributes as good or bad credit risks. Comes in two formats (one all numeric). Also comes with a cost matrix', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1000, 'num_features': 20, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Other', 'Marital Status', 'Age', 'Occupation'], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1994, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5NC77', 'creators': ['Hans Hofmann'], 'intro_paper': None, 'additional_info': {'summary': 'Two datasets are provided.  the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file \"german.data\".   \\r\\n \\r\\nFor algorithms that need numerical attributes, Strathclyde University produced the file \"german.data-numeric\".  This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables.   Several attributes that are ordered categorical (such as attribute 17) have been coded as integer.    This was the form used by StatLog.\\r\\n\\r\\nThis dataset requires use of a cost matrix (see below)\\r\\n\\r\\n ..... 1        2\\r\\n----------------------------\\r\\n  1   0        1\\r\\n-----------------------\\r\\n  2   5        0\\r\\n\\r\\n(1 = Good,  2 = Bad)\\r\\n\\r\\nThe rows represent the actual classification and the columns the predicted classification.\\r\\n\\r\\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Attribute 1:  (qualitative)      \\r\\n Status of existing checking account\\r\\n             A11 :      ... <    0 DM\\r\\n\\t       A12 : 0 <= ... <  200 DM\\r\\n\\t       A13 :      ... >= 200 DM / salary assignments for at least 1 year\\r\\n               A14 : no checking account\\r\\n\\r\\nAttribute 2:  (numerical)\\r\\n\\t      Duration in month\\r\\n\\r\\nAttribute 3:  (qualitative)\\r\\n\\t      Credit history\\r\\n\\t      A30 : no credits taken/ all credits paid back duly\\r\\n              A31 : all credits at this bank paid back duly\\r\\n\\t      A32 : existing credits paid back duly till now\\r\\n              A33 : delay in paying off in the past\\r\\n\\t      A34 : critical account/  other credits existing (not at this bank)\\r\\n\\r\\nAttribute 4:  (qualitative)\\r\\n\\t      Purpose\\r\\n\\t      A40 : car (new)\\r\\n\\t      A41 : car (used)\\r\\n\\t      A42 : furniture/equipment\\r\\n\\t      A43 : radio/television\\r\\n\\t      A44 : domestic appliances\\r\\n\\t      A45 : repairs\\r\\n\\t      A46 : education\\r\\n\\t      A47 : (vacation - does not exist?)\\r\\n\\t      A48 : retraining\\r\\n\\t      A49 : business\\r\\n\\t      A410 : others\\r\\n\\r\\nAttribute 5:  (numerical)\\r\\n\\t      Credit amount\\r\\n\\r\\nAttibute 6:  (qualitative)\\r\\n\\t      Savings account/bonds\\r\\n\\t      A61 :          ... <  100 DM\\r\\n\\t      A62 :   100 <= ... <  500 DM\\r\\n\\t      A63 :   500 <= ... < 1000 DM\\r\\n\\t      A64 :          .. >= 1000 DM\\r\\n              A65 :   unknown/ no savings account\\r\\n\\r\\nAttribute 7:  (qualitative)\\r\\n\\t      Present employment since\\r\\n\\t      A71 : unemployed\\r\\n\\t      A72 :       ... < 1 year\\r\\n\\t      A73 : 1  <= ... < 4 years  \\r\\n\\t      A74 : 4  <= ... < 7 years\\r\\n\\t      A75 :       .. >= 7 years\\r\\n\\r\\nAttribute 8:  (numerical)\\r\\n\\t      Installment rate in percentage of disposable income\\r\\n\\r\\nAttribute 9:  (qualitative)\\r\\n\\t      Personal status and sex\\r\\n\\t      A91 : male   : divorced/separated\\r\\n\\t      A92 : female : divorced/separated/married\\r\\n              A93 : male   : single\\r\\n\\t      A94 : male   : married/widowed\\r\\n\\t      A95 : female : single\\r\\n\\r\\nAttribute 10: (qualitative)\\r\\n\\t      Other debtors / guarantors\\r\\n\\t      A101 : none\\r\\n\\t      A102 : co-applicant\\r\\n\\t      A103 : guarantor\\r\\n\\r\\nAttribute 11: (numerical)\\r\\n\\t      Present residence since\\r\\n\\r\\nAttribute 12: (qualitative)\\r\\n\\t      Property\\r\\n\\t      A121 : real estate\\r\\n\\t      A122 : if not A121 : building society savings agreement/ life insurance\\r\\n              A123 : if not A121/A122 : car or other, not in attribute 6\\r\\n\\t      A124 : unknown / no property\\r\\n\\r\\nAttribute 13: (numerical)\\r\\n\\t      Age in years\\r\\n\\r\\nAttribute 14: (qualitative)\\r\\n\\t      Other installment plans \\r\\n\\t      A141 : bank\\r\\n\\t      A142 : stores\\r\\n\\t      A143 : none\\r\\n\\r\\nAttribute 15: (qualitative)\\r\\n\\t      Housing\\r\\n\\t      A151 : rent\\r\\n\\t      A152 : own\\r\\n\\t      A153 : for free\\r\\n\\r\\nAttribute 16: (numerical)\\r\\n              Number of existing credits at this bank\\r\\n\\r\\nAttribute 17: (qualitative)\\r\\n\\t      Job\\r\\n\\t      A171 : unemployed/ unskilled  - non-resident\\r\\n\\t      A172 : unskilled - resident\\r\\n\\t      A173 : skilled employee / official\\r\\n\\t      A174 : management/ self-employed/\\r\\n\\t\\t     highly qualified employee/ officer\\r\\n\\r\\nAttribute 18: (numerical)\\r\\n\\t      Number of people being liable to provide maintenance for\\r\\n\\r\\nAttribute 19: (qualitative)\\r\\n\\t      Telephone\\r\\n\\t      A191 : none\\r\\n\\t      A192 : yes, registered under the customers name\\r\\n\\r\\nAttribute 20: (qualitative)\\r\\n\\t      foreign worker\\r\\n\\t      A201 : yes\\r\\n\\t      A202 : no\\r\\n', 'citation': None}}\n",
      "           name     role         type     demographic  \\\n",
      "0    Attribute1  Feature  Categorical            None   \n",
      "1    Attribute2  Feature      Integer            None   \n",
      "2    Attribute3  Feature  Categorical            None   \n",
      "3    Attribute4  Feature  Categorical            None   \n",
      "4    Attribute5  Feature      Integer            None   \n",
      "5    Attribute6  Feature  Categorical            None   \n",
      "6    Attribute7  Feature  Categorical           Other   \n",
      "7    Attribute8  Feature      Integer            None   \n",
      "8    Attribute9  Feature  Categorical  Marital Status   \n",
      "9   Attribute10  Feature  Categorical            None   \n",
      "10  Attribute11  Feature      Integer            None   \n",
      "11  Attribute12  Feature  Categorical            None   \n",
      "12  Attribute13  Feature      Integer             Age   \n",
      "13  Attribute14  Feature  Categorical            None   \n",
      "14  Attribute15  Feature  Categorical           Other   \n",
      "15  Attribute16  Feature      Integer            None   \n",
      "16  Attribute17  Feature  Categorical      Occupation   \n",
      "17  Attribute18  Feature      Integer            None   \n",
      "18  Attribute19  Feature       Binary            None   \n",
      "19  Attribute20  Feature       Binary           Other   \n",
      "20        class   Target       Binary            None   \n",
      "\n",
      "                                          description   units missing_values  \n",
      "0                 Status of existing checking account    None             no  \n",
      "1                                            Duration  months             no  \n",
      "2                                      Credit history    None             no  \n",
      "3                                             Purpose    None             no  \n",
      "4                                       Credit amount    None             no  \n",
      "5                               Savings account/bonds    None             no  \n",
      "6                            Present employment since    None             no  \n",
      "7   Installment rate in percentage of disposable i...    None             no  \n",
      "8                             Personal status and sex    None             no  \n",
      "9                          Other debtors / guarantors    None             no  \n",
      "10                            Present residence since    None             no  \n",
      "11                                           Property    None             no  \n",
      "12                                                Age   years             no  \n",
      "13                            Other installment plans    None             no  \n",
      "14                                            Housing    None             no  \n",
      "15            Number of existing credits at this bank    None             no  \n",
      "16                                                Job    None             no  \n",
      "17  Number of people being liable to provide maint...    None             no  \n",
      "18                                          Telephone    None             no  \n",
      "19                                     foreign worker    None             no  \n",
      "20                                  1 = Good, 2 = Bad    None             no  \n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    " \n",
    "# metadata \n",
    "print(statlog_german_credit_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(statlog_german_credit_data.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c0ee5-8ecb-4b91-9f84-57b1bca203a6",
   "metadata": {},
   "source": [
    "# Reload the clean dataset automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058faf5d-852a-43f2-8785-9d786ddebc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset_mappingv1(): \n",
    "    statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "    \n",
    "    # data (as pandas dataframes) \n",
    "    X = statlog_german_credit_data.data.features \n",
    "    y = statlog_german_credit_data.data.targets\n",
    "    \n",
    "    # Mapping dictionaries for each qualitative attribute\n",
    "    mapping_1 = {\n",
    "        'A11': '< 0 DM',\n",
    "        'A12': '0 <= ... < 200 DM',\n",
    "        'A13': '>= 200 DM / salary assignments for at least 1 year',\n",
    "        'A14': 'no checking account'\n",
    "    }\n",
    "    \n",
    "    mapping_3 = {\n",
    "        'A30': 'no credits taken/ all credits paid back duly',\n",
    "        'A31': 'all credits at this bank paid back duly',\n",
    "        'A32': 'existing credits paid back duly till now',\n",
    "        'A33': 'delay in paying off in the past',\n",
    "        'A34': 'critical account/ other credits existing (not at this bank)'\n",
    "    }\n",
    "    \n",
    "    mapping_4 = {\n",
    "        'A40': 'car (new)',\n",
    "        'A41': 'car (used)',\n",
    "        'A42': 'furniture/equipment',\n",
    "        'A43': 'radio/television',\n",
    "        'A44': 'domestic appliances',\n",
    "        'A45': 'repairs',\n",
    "        'A46': 'education',\n",
    "        'A47': '(vacation - does not exist?)',\n",
    "        'A48': 'retraining',\n",
    "        'A49': 'business',\n",
    "        'A410': 'others'\n",
    "    }\n",
    "    \n",
    "    mapping_6 = {\n",
    "        'A61': '< 100 DM',\n",
    "        'A62': '100 <= ... < 500 DM',\n",
    "        'A63': '500 <= ... < 1000 DM',\n",
    "        'A64': '>= 1000 DM',\n",
    "        'A65': 'unknown/ no savings account'\n",
    "    }\n",
    "    \n",
    "    mapping_7 = {\n",
    "        'A71': 'unemployed',\n",
    "        'A72': '< 1 year',\n",
    "        'A73': '1 <= ... < 4 years',\n",
    "        'A74': '4 <= ... < 7 years',\n",
    "        'A75': '>= 7 years'\n",
    "    }\n",
    "    \n",
    "    mapping_9 = {\n",
    "        'A91': 'male: divorced/separated',\n",
    "        'A92': 'female: divorced/separated/married',\n",
    "        'A93': 'male: single',\n",
    "        'A94': 'male: married/widowed',\n",
    "        'A95': 'female: single'\n",
    "    }\n",
    "    \n",
    "    mapping_10 = {\n",
    "        'A101': 'none',\n",
    "        'A102': 'co-applicant',\n",
    "        'A103': 'guarantor'\n",
    "    }\n",
    "    \n",
    "    mapping_12 = {\n",
    "        'A121': 'real estate',\n",
    "        'A122': 'building society savings agreement/ life insurance',\n",
    "        'A123': 'car or other, not in attribute 6',\n",
    "        'A124': 'unknown / no property'\n",
    "    }\n",
    "    \n",
    "    mapping_14 = {\n",
    "        'A141': 'bank',\n",
    "        'A142': 'stores',\n",
    "        'A143': 'none'\n",
    "    }\n",
    "    \n",
    "    mapping_15 = {\n",
    "        'A151': 'rent',\n",
    "        'A152': 'own',\n",
    "        'A153': 'for free'\n",
    "    }\n",
    "    \n",
    "    mapping_17 = {\n",
    "        'A171': 'unemployed/ unskilled - non-resident',\n",
    "        'A172': 'unskilled - resident',\n",
    "        'A173': 'skilled employee / official',\n",
    "        'A174': 'management/ self-employed/ highly qualified employee/ officer'\n",
    "    }\n",
    "    \n",
    "    mapping_19 = {\n",
    "        'A191': 'none',\n",
    "        'A192': 'yes, registered under the customer\\'s name'\n",
    "    }\n",
    "    \n",
    "    mapping_20 = {\n",
    "        'A201': 'yes',\n",
    "        'A202': 'no'\n",
    "    }\n",
    "    \n",
    "    # Apply the mappings to the dataframe\n",
    "    for attribute, mapping in zip(['Attribute1', 'Attribute3', 'Attribute4', 'Attribute6', 'Attribute7', 'Attribute9', 'Attribute10', 'Attribute12', 'Attribute14', 'Attribute15', 'Attribute17', 'Attribute19', 'Attribute20'],\n",
    "                                   [mapping_1, mapping_3, mapping_4, mapping_6, mapping_7, mapping_9, mapping_10, mapping_12, mapping_14, mapping_15, mapping_17, mapping_19, mapping_20]):\n",
    "        X[attribute] = X[attribute].map(mapping)\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b673a2-c0a9-4ddc-b365-6005addaf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_clean_dataset_mappingv1()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42)\n",
    "print(X)\n",
    "\n",
    "for column in X.columns:\n",
    "    print(f\"Valeurs uniques dans {column}:\")\n",
    "    print(X[column].value_counts())\n",
    "    print(\"\\n\")\n",
    "for column in y.columns:\n",
    "    print(f\"Valeurs uniques dans {column}:\")\n",
    "    print(y[column].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83907cb1-804d-456d-bbfd-cc8b9ce9ff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute1</th>\n",
       "      <th>Attribute2</th>\n",
       "      <th>Attribute3</th>\n",
       "      <th>Attribute4</th>\n",
       "      <th>Attribute5</th>\n",
       "      <th>Attribute6</th>\n",
       "      <th>Attribute7</th>\n",
       "      <th>Attribute8</th>\n",
       "      <th>Attribute9</th>\n",
       "      <th>Attribute10</th>\n",
       "      <th>Attribute11</th>\n",
       "      <th>Attribute12</th>\n",
       "      <th>Attribute13</th>\n",
       "      <th>Attribute14</th>\n",
       "      <th>Attribute15</th>\n",
       "      <th>Attribute16</th>\n",
       "      <th>Attribute17</th>\n",
       "      <th>Attribute18</th>\n",
       "      <th>Attribute19</th>\n",
       "      <th>Attribute20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt; 0 DM</td>\n",
       "      <td>6</td>\n",
       "      <td>critical account/ other credits existing (not ...</td>\n",
       "      <td>radio/television</td>\n",
       "      <td>1169</td>\n",
       "      <td>unknown/ no savings account</td>\n",
       "      <td>&gt;= 7 years</td>\n",
       "      <td>4</td>\n",
       "      <td>male: single</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>real estate</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled employee / official</td>\n",
       "      <td>1</td>\n",
       "      <td>yes, registered under the customer's name</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 &lt;= ... &lt; 200 DM</td>\n",
       "      <td>48</td>\n",
       "      <td>existing credits paid back duly till now</td>\n",
       "      <td>radio/television</td>\n",
       "      <td>5951</td>\n",
       "      <td>&lt; 100 DM</td>\n",
       "      <td>1 &lt;= ... &lt; 4 years</td>\n",
       "      <td>2</td>\n",
       "      <td>female: divorced/separated/married</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>real estate</td>\n",
       "      <td>22</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled employee / official</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Attribute1  Attribute2  \\\n",
       "0             < 0 DM           6   \n",
       "1  0 <= ... < 200 DM          48   \n",
       "\n",
       "                                          Attribute3        Attribute4  \\\n",
       "0  critical account/ other credits existing (not ...  radio/television   \n",
       "1           existing credits paid back duly till now  radio/television   \n",
       "\n",
       "   Attribute5                   Attribute6          Attribute7  Attribute8  \\\n",
       "0        1169  unknown/ no savings account          >= 7 years           4   \n",
       "1        5951                     < 100 DM  1 <= ... < 4 years           2   \n",
       "\n",
       "                           Attribute9 Attribute10  Attribute11  Attribute12  \\\n",
       "0                        male: single        none            4  real estate   \n",
       "1  female: divorced/separated/married        none            2  real estate   \n",
       "\n",
       "   Attribute13 Attribute14 Attribute15  Attribute16  \\\n",
       "0           67        none         own            2   \n",
       "1           22        none         own            1   \n",
       "\n",
       "                   Attribute17  Attribute18  \\\n",
       "0  skilled employee / official            1   \n",
       "1  skilled employee / official            1   \n",
       "\n",
       "                                 Attribute19 Attribute20  \n",
       "0  yes, registered under the customer's name         yes  \n",
       "1                                       none         yes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f1326f-0708-4197-b9b2-3a4154ff019a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class\n",
       "0      1\n",
       "1      2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995d88ed-64ae-4200-878f-82d6c119cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_text(row): \n",
    "    return (\n",
    "        f\"Applicant with a checking account status: {row['Attribute1']}, \"\n",
    "        f\"loan duration: {row['Attribute2']} months, \"\n",
    "        f\"credit history: {row['Attribute3']}, \"\n",
    "        f\"loan purpose: {row['Attribute4']}, \"\n",
    "        f\"credit amount: {row['Attribute5']}, \"\n",
    "        f\"savings account/bonds: {row['Attribute6']}, \"\n",
    "        f\"employment since: {row['Attribute7']}, \"\n",
    "        f\"installment rate as percentage of disposable income: {row['Attribute8']}, \"\n",
    "        f\"personal status and sex: {row['Attribute9']}, \"\n",
    "        f\"other debtors/guarantors: {row['Attribute10']}, \"\n",
    "        f\"present residence since: {row['Attribute11']}, \"\n",
    "        f\"property: {row['Attribute12']}, \"\n",
    "        f\"age: {row['Attribute13']} years, \"\n",
    "        f\"other installment plans: {row['Attribute14']}, \"\n",
    "        f\"housing: {row['Attribute15']}, \"\n",
    "        f\"existing credits at this bank: {row['Attribute16']}, \"\n",
    "        f\"job: {row['Attribute17']}, \"\n",
    "        f\"number of people liable for maintenance: {row['Attribute18']}, \"\n",
    "        f\"telephone: {'yes' if row['Attribute19'] == 1 else 'no'}, \"\n",
    "        f\"foreign worker: {'yes' if row['Attribute20'] == 1 else 'no'}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce1201-6523-4df6-8757-4e764cf158d1",
   "metadata": {},
   "source": [
    "# Analysis of Standard Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cee6e6-6c35-4083-972c-89272e017a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets\n",
    "\n",
    "# Convert categorical variables to dummy/indicator variables if necessary\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the models\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Train the models\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "logreg_pred = logreg.predict(X_test)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_pred)\n",
    "logreg_f1 = f1_score(y_test, logreg_pred, average='weighted')\n",
    "logreg_confusion = confusion_matrix(y_test, logreg_pred)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_f1 = f1_score(y_test, rf_pred, average='weighted')\n",
    "rf_confusion = confusion_matrix(y_test, rf_pred)\n",
    "\n",
    "# Print the metrics in a tabular form\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Accuracy': [logreg_accuracy, rf_accuracy],\n",
    "    'F1 Score': [logreg_f1, rf_f1],\n",
    "    'Confusion Matrix': [logreg_confusion, rf_confusion]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(logreg_confusion, annot=True, fmt='d', cmap='Purples', ax=ax[0])\n",
    "ax[0].set_title('Logistic Regression Confusion Matrix')\n",
    "sns.heatmap(rf_confusion, annot=True, fmt='d', cmap='Purples', ax=ax[1])\n",
    "ax[1].set_title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba73e7-7094-46ab-9af4-00ccd26653c4",
   "metadata": {},
   "source": [
    "# Analysis of GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e9621-6697-4018-8ff9-a9f10662cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the credit risk based on the following description as 'Good' or as 'Bad': {description}\\n\\nClassification:\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\", #gpt-3.5-turbo\n",
    "        max_tokens = 10, #max words\n",
    "        temperature= 0.3\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    print(description)\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719cecc-a542-4842-9003-45c261355ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e609ee-0585-4f35-bf16-7a41a43fbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "llm_confusion = np.array(llm_confusion) \n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "\n",
    "sns.heatmap(llm_confusion, annot=True, fmt='d', cmap='Purples', xticklabels=['Good', 'Bad'], yticklabels=['Good', 'Bad'])\n",
    "#sns.heatmap(llm_confusion[0] + llm_confusion[1], annot=True, fmt='d', cmap='Purples', xticklabels=['Good', 'Bad'], yticklabels=['Good', 'Bad'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('GPT 3.5 Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a362715-7d4c-4494-8ddf-f82dbec3b531",
   "metadata": {},
   "source": [
    "# Bias Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736da082-f1a6-493a-be84-2d581ed65cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predo(data):\n",
    "    s = (data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    pre_data = data.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        pre_data[col] = label_encoder.fit_transform(data[col])\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] <= 45] = 0\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] > 45]= 1\n",
    "\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 2] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 3] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 5] = 1  # female\n",
    "    return pd.DataFrame(pre_data.values.tolist()) \n",
    "    \n",
    "mean_list = ['Status of existing checking account', 'Duration in month', 'Credit history', 'Purpose',\n",
    "             'Credit amount', 'Savings account/bonds', 'Present employment since',\n",
    "             'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
    "             ' Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',\n",
    "             'Other installment plans', 'Housing', 'Number of existing credits at this bank' ,'Job',\n",
    "             'Number of people being liable to provide maintenance for' , 'Telephone' , 'foreign worker',\n",
    "             'target']\n",
    "\n",
    "train = predo(train)\n",
    "test = predo(test)\n",
    "\n",
    "train.columns = mean_list\n",
    "test.columns = mean_list\n",
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212764b1-df59-4bac-b4b7-56f85a274300",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data bias test'''\n",
    "\n",
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "\n",
    "train_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=train, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "\n",
    "'''method bias test'''\n",
    "\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "print('down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b3563-e039-4630-bdde-2586f79fd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_results = {\n",
    "    'Attribute': ['Foreign Worker', 'Age', 'Sex', 'Foreign Worker', 'Age', 'Sex'],\n",
    "    'DI Value': [1.305, 0.9473684210526315, 0.7380952380952381, 1.2809897692124672, 1.1463128602663486, 0.9313997662185856],  \n",
    "    'Dataset': ['Test', 'Test', 'Test', 'Train', 'Train', 'Train']\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(di_results)\n",
    "\n",
    "# Print the DataFrame to verify its correctness\n",
    "print(df)\n",
    "\n",
    "test_data = df[df['Dataset'] == 'Test']\n",
    "train_data = df[df['Dataset'] == 'Train']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Position of bars on the x-axis\n",
    "indices = range(len(test_data))\n",
    "\n",
    "# Plotting both test and train data\n",
    "rects1 = ax.barh([x - width/2 for x in indices], test_data['DI Value'], width, label='Test', color='mediumpurple')\n",
    "rects2 = ax.barh([x + width/2 for x in indices], train_data['DI Value'], width, label='Train', color='plum')\n",
    "\n",
    "# Adding a vertical line at x = 1\n",
    "ax.axvline(x=1, color='red', linestyle='--', label='No Bias Threshold')\n",
    "\n",
    "# Labeling and aesthetics\n",
    "ax.set_xlabel('Disparate Impact (DI)')\n",
    "ax.set_title('DI by Attribute and Dataset')\n",
    "ax.set_yticks(indices)\n",
    "ax.set_yticklabels(test_data['Attribute'])\n",
    "ax.legend()\n",
    "\n",
    "# Function to add labels on the bars\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        width = rect.get_width()\n",
    "        ax.annotate(f'{width:.2f}',\n",
    "                    xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                    xytext=(3, 0),  # 3 points horizontal offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center')\n",
    "\n",
    "# Add labels to the bars\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639f4a3-1344-4174-8ae3-839e2334c793",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning, library optuna to increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5a3cf-1929-434c-ae6d-d6efb94a08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72d244-d8e6-4c36-8988-a5c69d31cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25376352-136d-4472-8634-0c6150626409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggesting parameters\n",
    "    temperature = trial.suggest_float('temperature', 0.1, 1.0)\n",
    "    top_p = trial.suggest_float('top_p', 0.1, 1.0)\n",
    "    max_tokens = trial.suggest_int('max_tokens', 5, 50)\n",
    "    frequency_penalty = trial.suggest_float('frequency_penalty', 0.0, 2.0)\n",
    "    presence_penalty = trial.suggest_float('presence_penalty', 0.0, 2.0)\n",
    "\n",
    "    # Redefining the classifier with new parameters\n",
    "    def classify_with_gpt(description):\n",
    "        api_key = 'x'\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        prompt = f\"Classify the credit risk based on the following description as 'Good' or as 'Bad': {description}\\n\\nClassification:\"\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        \n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "        return 'Good' if 'Good' in response else 'Bad'\n",
    "\n",
    "    # Predicting using the classifier\n",
    "    predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "    predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3664a3-03d6-46d7-b7dc-bd4a00a641c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10, timeout=600)  # Run for 10 trials or stop after 600 seconds\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99966534-ec28-46ee-8849-7e59f187afbb",
   "metadata": {},
   "source": [
    "# Focus on Gender Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe83e57-11a9-4357-9cfe-ec1103688f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predo(data):\n",
    "    s = (data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    pre_data = data.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        pre_data[col] = label_encoder.fit_transform(data[col])\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] <= 45] = 0\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] > 45]= 1\n",
    "\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 2] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 3] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 5] = 1  # female\n",
    "    return pd.DataFrame(pre_data.values.tolist()) \n",
    "    \n",
    "mean_list = ['Status of existing checking account', 'Duration in month', 'Credit history', 'Purpose',\n",
    "             'Credit amount', 'Savings account/bonds', 'Present employment since',\n",
    "             'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
    "             ' Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',\n",
    "             'Other installment plans', 'Housing', 'Number of existing credits at this bank' ,'Job',\n",
    "             'Number of people being liable to provide maintenance for' , 'Telephone' , 'foreign worker',\n",
    "             'target']\n",
    "\n",
    "train = predo(train)\n",
    "test = predo(test)\n",
    "\n",
    "train.columns = mean_list\n",
    "test.columns = mean_list\n",
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d4f03-745e-447f-b1d5-db0ef96a07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374e377-d4d4-4c9f-97a2-41f53dc8f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Evaluate the financial details provided and determine the credit risk as only 'Good' or 'Bad' based solely on financial metrics. Avoid considering personal demographics unrelated to creditworthiness. For instance, if the provided data shows consistent income, no existing debts, and asset ownership, the decision should be 'Good'. Please focus strictly on the financial profile provided. Financial Profile: {description}\\nYou can only answer with 'Good' or 'Bad'\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\", #model name\n",
    "        max_tokens = 45, #max words\n",
    "        temperature= 0.74,\n",
    "        top_p = 0.17,\n",
    "        frequency_penalty = 0.45,\n",
    "        presence_penalty = 1.37\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667e21d-6cfb-41e7-92f1-cb6b809a2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]\n",
    "\n",
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac76a2-78a2-4c92-816b-f5863e3a07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Evaluate the financial details provided and determine the credit risk as only 'Good' or 'Bad' based solely on financial metrics. Avoid considering personal demographics unrelated to creditworthiness. Financial Profile: {description}\\nYou can only answer with 'Good' or 'Bad'\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\", #model name\n",
    "        max_tokens = 45, #max words\n",
    "        temperature= 0.74,\n",
    "        top_p = 0.17,\n",
    "        frequency_penalty = 0.45,\n",
    "        presence_penalty = 1.37\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d483f1-0149-473d-850e-a4069046d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]\n",
    "\n",
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c9919-252c-4592-a151-6678df8260f7",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2feb1-6f0e-425f-a136-32bab13b09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the initial dataset without mappings for the analysis of correlation matrix\n",
    "\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "    \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494dbd0-3b32-4f29-ba27-dfda012387e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X.select_dtypes(include='object').columns\n",
    "df_encoded = pd.get_dummies(X, columns=categorical_columns)\n",
    "print(df_encoded)\n",
    "\n",
    "# Calculer la matrice de corrlation\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Afficher la matrice de corrlation avec Seaborn\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e641a4-97eb-4983-8d64-69c5590ecaa3",
   "metadata": {},
   "source": [
    "# GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fc5f5-60f3-4b64-84ae-7fd1121e0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the credit risk based on the following description as Good or as Bad: {description}\\nYou can only answer with 'Good' or 'Bad'\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\", #model name\n",
    "        max_tokens = 5, #max words\n",
    "        temperature= 0.3\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f747f9-7814-4794-bcfd-967e3c4904d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the credit risk based on the following description as Good or as Bad: {description}\\nYou can only answer with 'Good' or 'Bad'\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\", #model name\n",
    "        max_tokens = 45, #max words\n",
    "        temperature= 0.74,\n",
    "        top_p = 0.17,\n",
    "        frequency_penalty = 0.45,\n",
    "        presence_penalty = 1.37\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2f9cb-c2b3-4e73-a2cd-ac828717b42d",
   "metadata": {},
   "source": [
    "### Bias measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a642b-4699-4ca9-9725-2979b5f4cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predo(data):\n",
    "    s = (data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    pre_data = data.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        pre_data[col] = label_encoder.fit_transform(data[col])\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] <= 45] = 0\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] > 45]= 1\n",
    "\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 2] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 3] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 5] = 1  # female\n",
    "    return pd.DataFrame(pre_data.values.tolist()) \n",
    "    \n",
    "mean_list = ['Status of existing checking account', 'Duration in month', 'Credit history', 'Purpose',\n",
    "             'Credit amount', 'Savings account/bonds', 'Present employment since',\n",
    "             'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
    "             ' Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',\n",
    "             'Other installment plans', 'Housing', 'Number of existing credits at this bank' ,'Job',\n",
    "             'Number of people being liable to provide maintenance for' , 'Telephone' , 'foreign worker',\n",
    "             'target']\n",
    "\n",
    "train = predo(train)\n",
    "test = predo(test)\n",
    "\n",
    "train.columns = mean_list\n",
    "test.columns = mean_list\n",
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81c746-3fcf-44c2-a489-08d638172992",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data bias test'''\n",
    "\n",
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(test_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "\n",
    "train_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=train, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(train_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "print('DI:', text_res.disparate_impact())\n",
    "\n",
    "\n",
    "'''method bias test'''\n",
    "\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex','Age in years','foreign worker'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'foreign worker':0}], privileged_groups=[{'foreign worker':1}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Age in years':1}], privileged_groups=[{'Age in years':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n",
    "\n",
    "print('down')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec98525-1236-4ca5-81f0-2c76e85c2ee5",
   "metadata": {},
   "source": [
    "## Other tests with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee54db-97f2-4cf0-8b1e-62b0c85e5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Evaluate the creditworthiness of a customer with the following financial profile and respond with only Good or Bad. For example, a client described as having a stable income, no previous debts, and owning property should be classified as Good. Client:{description}\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\", #model name\n",
    "        max_tokens = 5, #max words\n",
    "        temperature= 0.3\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da74e9a-056a-4065-b990-76e0e304e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggesting parameters\n",
    "    temperature = trial.suggest_float('temperature', 0.1, 1.0)\n",
    "    top_p = trial.suggest_float('top_p', 0.1, 1.0)\n",
    "    max_tokens = trial.suggest_int('max_tokens', 5, 50)\n",
    "    frequency_penalty = trial.suggest_float('frequency_penalty', 0.0, 2.0)\n",
    "    presence_penalty = trial.suggest_float('presence_penalty', 0.0, 2.0)\n",
    "\n",
    "    # Redefining the classifier with new parameters\n",
    "    def classify_with_gpt(description):\n",
    "        api_key = 'x'\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        prompt = f\"Classify the credit risk based on the following description as 'Good' or as 'Bad': {description}\\n\\nClassification:\"\n",
    "        \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4o\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty\n",
    "        )\n",
    "        \n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "        return 'Good' if 'Good' in response else 'Bad'\n",
    "\n",
    "    # Predicting using the classifier\n",
    "    predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "    predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b8ea3-4359-477e-bf72-85c6d5e11955",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10, timeout=600)  # Run for 10 trials or stop after 600 seconds\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebc6f2-e1c0-45f2-9799-9dcb7beeb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with best parameter \n",
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the credit risk based on the following description as Good or as Bad: {description}\\nYou can only answer with 'Good' or 'Bad'\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\", #model name\n",
    "        max_tokens = 32, #max words\n",
    "        temperature= 0.5,\n",
    "        top_p = 0.74,\n",
    "        frequency_penalty = 0.18,\n",
    "        presence_penalty = 0.98\n",
    "    )\n",
    "    return chat_completion\n",
    "    \n",
    "\n",
    "# Function to classify using GPT\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "#X_test_descriptions = X_test_descriptions[:5] \n",
    "\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "#print(predictions)\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36b378-b4cb-458e-b20a-e8b9505a381c",
   "metadata": {},
   "source": [
    "## Gender bias focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab122e-bf60-4459-bda3-abcdeeaaa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def predo(data):\n",
    "    s = (data.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    pre_data = data.copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        pre_data[col] = label_encoder.fit_transform(data[col])\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] <= 45] = 0\n",
    "    pre_data['Attribute13'][pre_data['Attribute13'] > 45]= 1\n",
    "\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 2] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 3] = 0  # male\n",
    "    pre_data['Attribute9'][pre_data['Attribute9'] == 5] = 1  # female\n",
    "    return pd.DataFrame(pre_data.values.tolist()) \n",
    "    \n",
    "mean_list = ['Status of existing checking account', 'Duration in month', 'Credit history', 'Purpose',\n",
    "             'Credit amount', 'Savings account/bonds', 'Present employment since',\n",
    "             'Installment rate in percentage of disposable income', 'Personal status and sex',\n",
    "             ' Other debtors / guarantors', 'Present residence since', 'Property', 'Age in years',\n",
    "             'Other installment plans', 'Housing', 'Number of existing credits at this bank' ,'Job',\n",
    "             'Number of people being liable to provide maintenance for' , 'Telephone' , 'foreign worker',\n",
    "             'target']\n",
    "\n",
    "train = predo(train)\n",
    "test = predo(test)\n",
    "\n",
    "train.columns = mean_list\n",
    "test.columns = mean_list\n",
    "result = test.copy()\n",
    "result['target'] = predictions.reset_index()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a12bc2-c9f3-4eb5-a1c3-4ad390e11f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=test, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "result_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=2, df=result, label_names=['target'], protected_attribute_names=['Personal status and sex'])\n",
    "\n",
    "metric = ClassificationMetric(test_data, result_data, unprivileged_groups=[{'Personal status and sex':1}], privileged_groups=[{'Personal status and sex':0}])\n",
    "text_res = MetricTextExplainer(metric)\n",
    "\n",
    "print('EOD:', text_res.equal_opportunity_difference())\n",
    "print('ERR:', text_res.average_odds_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e02377-7463-4264-b099-f6e2cd7fe62a",
   "metadata": {},
   "source": [
    "# Testing prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f8dc6-7568-482e-b9a1-eec5ff447c36",
   "metadata": {},
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285c647-9553-4d09-b57e-e6d67a596be8",
   "metadata": {},
   "source": [
    "max_tokens = 45, #max words\n",
    "        temperature= 0.74,\n",
    "        top_p = 0.17,\n",
    "        frequency_penalty = 0.45,\n",
    "        presence_penalty = 1.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9be38c7-0414-43da-abf6-bdb91776f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.706060606060606\n",
      "F1 Score: 0.6490992893823093\n",
      "Confusion: [[649  45]\n",
      " [246  50]]\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Your task is to Classify the credit risk of individuals based on detailed financial and demographic data as 'Good' or as 'Bad'. Ensure fairness and avoid bias based on age, gender, or nationality. \\n The Dataset used is 'Statlog German Credit Data', assessing creditworthiness using 20 attributes related to financial stability and history. Directly following this instruction, analyze the provided data attributes and determine the credit risk as either 'Good' or 'Bad', your answer should only be 'Good' or 'Bad', nothing else.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": description\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        max_tokens=45,  \n",
    "        temperature=0.74, \n",
    "        top_p = 0.17,\n",
    "        frequency_penalty = 0.45,\n",
    "        presence_penalty = 1.37\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    if output not in ['Good', 'Bad']:\n",
    "        raise ValueError(f\"Unexpected output: {output}\")\n",
    "    return output\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7d3c0-d04a-41d9-8d8b-079b87ef603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Task: Accurately classify the credit risk of individuals as Good or Bad based on the detailed description provided below. Your response must strictly adhere to this binary classification.Dataset: Statlog (German Credit Data), comprising attributes relevant to determining creditworthiness, including demographic information, financial history, and employment details. Guidelines: 1. Your classification must reflect a rigorous analysis of the financial and demographic data without bias.2. Do not provide explanations or justifications. Your response should categorically state Good or Bad based on the credit risk assessment, do not explain the reason behind the classification.3. Ensure fairness and impartiality in every assessment, avoiding discrimination based on age, gender, or nationality.Directly following this instruction, analyze the provided data attributes and determine the credit risk as either Good or Bad.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": description\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        max_tokens=2,  # Slightly increased to allow for the correct response\n",
    "        temperature=0.0  # Lower temperature for deterministic output\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    if output not in ['Good', 'Bad']:\n",
    "        raise ValueError(f\"Unexpected output: {output}\")\n",
    "    return output\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022a3da-6285-4e62-a1e0-2016f403bd79",
   "metadata": {},
   "source": [
    "## GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdfb042c-711e-4746-a097-c269d68ca6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6515151515151515\n",
      "F1 Score: 0.5906810544444271\n",
      "Confusion: [[616  78]\n",
      " [267  29]]\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Given the input below, classify the credit risk as either 'Good' or 'Bad'. Your answer must strictly be 'Good' or 'Bad'. Do not provide any additional text or explanation.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": description\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2,  # Slightly increased to allow for the correct response\n",
    "        temperature=0.0  # Lower temperature for deterministic output\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    if output not in ['Good', 'Bad']:\n",
    "        raise ValueError(f\"Unexpected output: {output}\")\n",
    "    return output\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb37b80-760f-40ea-a764-a485da99f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.693939393939394\n",
      "F1 Score: 0.5831443412159858\n",
      "Confusion: [[682  12]\n",
      " [291   5]]\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(description):\n",
    "    api_key = 'x'  # my API KEY from my openAI playground\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", api_key))\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Your task is to Classify the credit risk of individuals based on detailed financial and demographic data as 'Good' or as 'Bad'. Ensure fairness and avoid bias based on age, gender, or nationality. \\n The Dataset used is 'Statlog German Credit Data', assessing creditworthiness using 20 attributes related to financial stability and history. Directly following this instruction, analyze the provided data attributes and determine the credit risk as either 'Good' or 'Bad', your answer should only be 'Good' or 'Bad', nothing else.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": description\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=39,  \n",
    "        temperature=0.66, \n",
    "        top_p = 0.95,\n",
    "        frequency_penalty = 1.06,\n",
    "        presence_penalty = 1.38\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "def classify_with_gpt(description):\n",
    "    response = chat_with_gpt(description)\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    if output not in ['Good', 'Bad']:\n",
    "        raise ValueError(f\"Unexpected output: {output}\")\n",
    "    return output\n",
    "\n",
    "# Convert features to text and classify\n",
    "X_test_descriptions = X_test.apply(features_to_text, axis=1)\n",
    "predictions = X_test_descriptions.apply(classify_with_gpt)\n",
    "\n",
    "# Replace 'Good' as 1 and 'Bad' as 2\n",
    "predictions = predictions.replace({'Good': 1, 'Bad': 2})\n",
    "y_test = y_test.loc[predictions.index]\n",
    "\n",
    "# Evaluate the model\n",
    "llm_accuracy = accuracy_score(y_test, predictions)\n",
    "llm_f1 = f1_score(y_test, predictions, average='weighted')\n",
    "llm_confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", llm_accuracy)\n",
    "print(\"F1 Score:\", llm_f1)\n",
    "print('Confusion:', llm_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b0e33-3800-44ce-a3fe-f318510e553a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
